{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ LoRA å’Œ Hugging Face è¿›è¡Œé«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "åœ¨è¿™ä¸ª sagemaker ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•åº”ç”¨ [Low-Rank Adaptation of Large Language Models (LoRA)](https://arxiv.org/abs/2106.09685) æ¥å¾®è°ƒ BLOOMZ (BLOOM 70äº¿å‚æ•°ç‰ˆæœ¬æŒ‡ä»¤è°ƒä¼˜ç‰ˆæœ¬) åœ¨å•å—GPUä¸Šã€‚ æˆ‘ä»¬å°†åˆ©ç”¨ Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), ä»¥åŠ [PEFT](https://github.com/huggingface/peft). \n",
    "\n",
    "ä½ å°†å­¦åˆ°å¦‚ä½•ï¼š\n",
    "\n",
    "1. è®¾ç½®å¼€å‘ç¯å¢ƒ\n",
    "2. åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†\n",
    "3. åœ¨ Amazon SageMaker ä¸Šä½¿ç”¨ LoRA å’Œ bnb int-8 æ¥å¾®è°ƒ BLOOM\n",
    "4. å°†æ¨¡å‹éƒ¨ç½²åˆ° Amazon SageMaker ç«¯ç‚¹\n",
    "\n",
    "### ç®€ä»‹ï¼šPEFT æˆ–å‚æ•°é«˜æ•ˆå¾®è°ƒ\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), æˆ– Parameter Efficient Fine-tuningï¼Œæ˜¯ Hugging Face çš„ä¸€ä¸ªæ–°çš„å¼€æºåº“ï¼Œå¯ä»¥ä½¿é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (PLM) é«˜æ•ˆé€‚åº”å„ç§ä¸‹æ¸¸åº”ç”¨ç¨‹åºï¼Œè€Œæ— éœ€å¾®è°ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ã€‚ PEFT ç›®å‰åŒ…æ‹¬ä»¥ä¸‹æŠ€æœ¯ï¼š\n",
    "- LoRA:Â [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning:Â [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning:Â [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning:Â [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.26.0\" \"datasets[s3]==2.9.0\" sagemaker py7zr --upgrade --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœæ‚¨è¦åœ¨æœ¬åœ°ç¯å¢ƒä¸­ä½¿ç”¨ Sagemakerã€‚ æ‚¨éœ€è¦è®¿é—®å…·æœ‰ Sagemaker æ‰€éœ€æƒé™çš„ IAM è§’è‰²ã€‚ æ‚¨å¯ä»¥åœ¨ [æ­¤å¤„](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) æ‰¾åˆ°æ›´å¤šç›¸å…³ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†\n",
    "\n",
    "æˆ‘ä»¬å°†ä½¿ç”¨[samsum](https://huggingface.co/datasets/samsum)æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤§çº¦16kä¸ªå¸¦æœ‰æ‘˜è¦çš„ç±»ä¼¼ä¿¡ä½¿çš„å¯¹è¯çš„é›†åˆã€‚å¯¹è¯æ˜¯ç”±ç²¾é€šè‹±è¯­çš„è¯­è¨€å­¦å®¶åˆ›å»ºå¹¶è®°å½•ä¸‹æ¥çš„ã€‚\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"id\": \"13818513\",\n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "}\n",
    "```\n",
    "\n",
    "è¦åŠ è½½ `samsum` æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨ ğŸ¤—Hugging Face Datasets åº“ä¸­çš„ `load_dataset()` æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"samsum\", split=\"train\")\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset)}\")\n",
    "# Train dataset size: 14732"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬çš„è¾“å…¥ï¼ˆæ–‡æœ¬ï¼‰è½¬æ¢ä¸ºtoken IDã€‚ è¿™æ˜¯ç”± ğŸ¤— Transformers Tokenizer å®Œæˆçš„ã€‚ å¦‚æœæ‚¨ä¸ç¡®å®šè¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Œè¯·æŸ¥çœ‹æŠ±æŠ±è„¸è¯¾ç¨‹çš„**[ç¬¬ 6 ç« ](https://huggingface.co/course/chapter6/1?fw=tf)**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id=\"bigscience/bloomz-7b1\"\n",
    "\n",
    "# Load tokenizer of BLOOMZ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.model_max_length = 2048 # overwrite wrong value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦é¢„å¤„ç†æˆ‘ä»¬çš„æ•°æ®ã€‚æŠ½è±¡æ‘˜è¦æ˜¯ä¸€é¡¹æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ¨¡å‹å°†ä»¥æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆæ‘˜è¦ä½œä¸ºè¾“å‡ºã€‚æˆ‘ä»¬æƒ³äº†è§£è¾“å…¥å’Œè¾“å‡ºéœ€è¦å¤šé•¿æ—¶é—´æ‰èƒ½æœ‰æ•ˆåœ°æ‰¹å¤„ç†æ•°æ®ã€‚\n",
    "\n",
    "æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª' prompt_template 'ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥æ„é€ ä¸€ä¸ªæŒ‡ç¤ºæç¤ºç¬¦ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„' prompt_template 'æœ‰ä¸€ä¸ªå›ºå®šçš„å¼€å§‹å’Œç»“æŸï¼Œæˆ‘ä»¬çš„æ–‡æ¡£åœ¨ä¸­é—´ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦ç¡®ä¿â€œå›ºå®šâ€æ¨¡æ¿éƒ¨ä»¶+æ–‡æ¡£ä¸è¶…è¿‡æ¨¡å‹çš„æœ€å¤§é•¿åº¦ã€‚\n",
    "æˆ‘ä»¬åœ¨è®­ç»ƒå‰å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶å°†å…¶ä¿å­˜åˆ°ç£ç›˜ä¸Šï¼Œç„¶åä¸Šä¼ åˆ°S3ã€‚ä½ å¯ä»¥åœ¨ä½ çš„æœ¬åœ°æœºå™¨æˆ–CPUä¸Šè¿è¡Œè¿™ä¸ªæ­¥éª¤ï¼Œå¹¶å°†å®ƒä¸Šä¼ åˆ°[Hugging Face Hub](https://huggingface.co/docs/hub/datasets-overview)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "# custom instruct prompt start\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n{{summary}}{{eos_token}}\"\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = prompt_template.format(dialogue=sample[\"dialogue\"],\n",
    "                                            summary=sample[\"summary\"],\n",
    "                                            eos_token=tokenizer.eos_token)\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¤„ç†å®Œæ•°æ®é›†åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ–°çš„[æ–‡ä»¶ç³»ç»Ÿæ•´åˆ](https://huggingface.co/docs/datasets/filesystems)å°†æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šä¼ åˆ°S3ã€‚æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨' sess.default_bucket() 'ï¼Œå¦‚æœæ‚¨æƒ³å°†æ•°æ®é›†å­˜å‚¨åœ¨ä¸åŒçš„S3æ¡¶ä¸­ï¼Œè¯·è°ƒæ•´æ­¤è®¾ç½®ã€‚æˆ‘ä»¬å°†åœ¨åé¢çš„è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨S3è·¯å¾„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/samsum-sagemaker/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. åœ¨ Amazon SageMaker ä¸Šä½¿ç”¨ LoRA å’Œ bnb int-8 å¾®è°ƒ BLOOM\n",
    "\n",
    "é™¤äº† LoRA æŠ€æœ¯ï¼Œæˆ‘ä»¬è¿˜å°†ä½¿ç”¨ [bitsanbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration) é€šè¿‡Freezeæ–¹æ³•å°† LLM é‡åŒ–ä¸º int8ã€‚ è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°† BLOOMZ æ‰€éœ€çš„å†…å­˜å‡å°‘çº¦ 4 å€ã€‚\n",
    "\n",
    "æˆ‘ä»¬å‡†å¤‡äº†ä¸€ä¸ª [run_clm.py](./scripts/run_clm.py)ï¼Œå®ƒå®ç°äº†ä½¿ç”¨ PEFT æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚ å¦‚æœæ‚¨å¯¹å…¶å·¥ä½œåŸç†æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹ [ä½¿ç”¨ LoRA å’Œ Hugging Face è¿›è¡Œé«˜æ•ˆå¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒ](https://www.philschmid.de/fine-tune-flan-t5-peft) åšå®¢ï¼Œæˆ‘ä»¬åœ¨å…¶ä¸­è¯¦ç»†çš„è§£é‡Šäº†è®­ç»ƒè„šæœ¬ã€‚\n",
    "\n",
    "\n",
    "ä¸ºäº†åˆ›å»º sagemaker è®­ç»ƒä½œä¸šï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª HuggingFace ä¼°è®¡å™¨ã€‚ Estimator å¤„ç†ç«¯åˆ°ç«¯çš„ Amazon SageMaker è®­ç»ƒå’Œéƒ¨ç½²ä»»åŠ¡ã€‚ Estimator ç®¡ç†åŸºç¡€è®¾æ–½çš„ä½¿ç”¨ã€‚\n",
    "SagMaker è´Ÿè´£ä¸ºæˆ‘ä»¬å¯åŠ¨å’Œç®¡ç†æ‰€æœ‰å¿…éœ€çš„ ec2 å®ä¾‹ï¼Œæä¾›æ­£ç¡®çš„ huggingface å®¹å™¨ï¼Œä¸Šä¼ æä¾›çš„è„šæœ¬å¹¶å°†æ•°æ®ä»æˆ‘ä»¬çš„ S3 å­˜å‚¨æ¡¶ä¸‹è½½åˆ°ä½äºâ€œ/opt/ml/input/dataâ€çš„å®¹å™¨ä¸­ã€‚ ç„¶åï¼Œå®ƒé€šè¿‡è¿è¡Œå¼€å§‹è®­ç»ƒå·¥ä½œã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-peft-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                                # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training', # path where sagemaker will save training dataset\n",
    "  'epochs': 3,                                         # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                    # batch size for training\n",
    "  'lr': 2e-4,                                          # learning rate used during training\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.2xlarge', # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',            # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ç°åœ¨å¯ä»¥å¼€å§‹æˆ‘ä»¬çš„è®­ç»ƒå·¥ä½œï¼Œä½¿ç”¨ .fit() æ–¹æ³•å°†æˆ‘ä»¬çš„ S3 è·¯å¾„ä¼ é€’ç»™è®­ç»ƒè„šæœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®­ç»ƒç”¨äº†â€œ20632â€ç§’ï¼Œå¤§çº¦æ˜¯â€œ5.7â€å°æ—¶ã€‚ æˆ‘ä»¬ä½¿ç”¨çš„â€œml.g5.2xlargeâ€å®ä¾‹æ¯å°æ—¶æ”¶è´¹â€œ1.515 ç¾å…ƒâ€ã€‚ å› æ­¤ï¼Œè®­ç»ƒ BLOOMZ 7B çš„æ€»æˆæœ¬ä¸º 8.63 ç¾å…ƒã€‚ æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ spot å®ä¾‹æ¥é™ä½æˆæœ¬ï¼Œä½†æ˜¯é€šè¿‡ç­‰å¾…æˆ–é‡å¯å¯èƒ½ä¼šå¢åŠ è®­ç»ƒæ—¶é—´ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å°†æ¨¡å‹éƒ¨ç½²åˆ° Amazon SageMaker ç«¯ç‚¹\n",
    "\n",
    "å½“ä½¿ç”¨ `peft` è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæ‚¨é€šå¸¸ä¼šå¾—åˆ°é€‚é…å™¨æƒé‡ã€‚ æˆ‘ä»¬æ·»åŠ äº† `merge_and_unload()` æ–¹æ³•æ¥å°†åŸºç¡€æ¨¡å‹ä¸ adatper åˆå¹¶ï¼Œä»¥ä¾¿æ›´è½»æ¾åœ°éƒ¨ç½²æ¨¡å‹ã€‚ å› æ­¤æˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨ transformers åº“çš„ pipelines åŠŸèƒ½ã€‚\n",
    "\n",
    "æˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨æˆ‘ä»¬çš„ HuggingFace ä¼°è®¡å™¨å¯¹è±¡ä¸Šä½¿ç”¨â€œdeploy()â€æ¥éƒ¨ç½²æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä¼ é€’æˆ‘ä»¬æƒ³è¦çš„å®ä¾‹æ•°é‡å’Œå®ä¾‹ç±»å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=huggingface_estimator.model_data,\n",
    "   #model_data=\"s3://hf-sagemaker-inference/model.tar.gz\",  # Change to your model path\n",
    "   role=role, \n",
    "   transformers_version=\"4.26\", \n",
    "   pytorch_version=\"1.13\", \n",
    "   py_version=\"py39\",\n",
    "   model_server_workers=1\n",
    ")\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type= \"ml.g5.4xlarge\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker é€šè¿‡åˆ›å»º SageMaker ç«¯ç‚¹é…ç½®å’Œ SageMaker ç«¯ç‚¹æ¥å¯åŠ¨éƒ¨ç½²è¿‡ç¨‹ã€‚ ç«¯ç‚¹é…ç½®å®šä¹‰æ¨¡å‹å’Œå®ä¾‹ç±»å‹ã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬ä½¿ç”¨â€œæµ‹è¯•â€æ‹†åˆ†ä¸­çš„ç¤ºä¾‹è¿›è¡Œæµ‹è¯•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")\n",
    "\n",
    "# select a random test sample\n",
    "sample = test_dataset[randint(0,len(test_dataset))]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "fomatted_sample = {\n",
    "  \"inputs\": prompt_template.format(dialogue=sample[\"dialogue\"]),\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_new_tokens\": 100,\n",
    "  }\n",
    "}\n",
    "\n",
    "# predict\n",
    "res = predictor.predict(fomatted_sample)\n",
    "\n",
    "print(res[0][\"generated_text\"].split(\"Summary:\")[-1])\n",
    "# Kirsten and Alex are going bowling this Friday at 7 pm. They will meet up and then go together.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®©æˆ‘ä»¬å°†å…¶ä¸æµ‹è¯•ç»“æœè¿›è¡Œæ¯”è¾ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[\"summary\"])\n",
    "# Kirsten reminds Alex that the youth group meets this Friday at 7 pm and go bowling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åï¼Œæˆ‘ä»¬å†æ¬¡åˆ é™¤ç«¯ç‚¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
