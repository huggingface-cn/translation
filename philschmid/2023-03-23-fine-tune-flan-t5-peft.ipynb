{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ LoRA å’Œ Hugging Face é«˜æ•ˆè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹\n",
    "\n",
    "åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨[å¤§è¯­è¨€æ¨¡å‹ä½ç§©é€‚é…ï¼ˆLow-Rank Adaptation of Large Language Modelsï¼ŒLoRAï¼‰](https://arxiv.org/abs/2106.09685)æŠ€æœ¯åœ¨å• GPU ä¸Šå¾®è°ƒ 110 äº¿å‚æ•°çš„ FLAN-T5 XXL æ¨¡å‹ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨åˆ° Hugging Face çš„ [Transformers](https://huggingface.co/docs/transformers/index)ã€[Accelerate](https://huggingface.co/docs/accelerate/index) å’Œ [PEFT](https://github.com/huggingface/peft) åº“ã€‚\n",
    "\n",
    "é€šè¿‡æœ¬æ–‡ï¼Œä½ ä¼šå­¦åˆ°ï¼š\n",
    "\n",
    "1. å¦‚ä½•æ­å»ºå¼€å‘ç¯å¢ƒ\n",
    "2. å¦‚ä½•åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†\n",
    "3. å¦‚ä½•ä½¿ç”¨ LoRA å’Œ bnbï¼ˆå³bitsandbytesï¼‰ int-8 å¾®è°ƒ T5\n",
    "4. å¦‚ä½•è¯„ä¼° LoRA FLAN-T5 å¹¶å°†å…¶ç”¨äºæ¨ç†\n",
    "5. å¦‚ä½•æ¯”è¾ƒä¸åŒæ–¹æ¡ˆçš„æ€§ä»·æ¯”\n",
    "\n",
    "### å¿«é€Ÿå…¥é—¨ï¼šè½»é‡åŒ–å¾®è°ƒï¼ˆParameter Efficient Fine-Tuningï¼ŒPEFTï¼‰\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft) æ˜¯ Hugging Face çš„ä¸€ä¸ªæ–°çš„å¼€æºåº“ã€‚ä½¿ç”¨ PEFT åº“ï¼Œæ— éœ€å¾®è°ƒæ¨¡å‹çš„å…¨éƒ¨å‚æ•°ï¼Œå³å¯é«˜æ•ˆåœ°å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (Pre-trained Language Modelï¼ŒPLM) é€‚é…åˆ°å„ç§ä¸‹æ¸¸åº”ç”¨ã€‚PEFT ç›®å‰æ”¯æŒä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š\n",
    "\n",
    "- LoRAï¼š[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuningï¼š[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuningï¼š[GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuningï¼š[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "\n",
    "*æ³¨æ„ï¼šæœ¬æ•™ç¨‹æ˜¯åœ¨ g5.2xlarge AWS EC2 å®ä¾‹ä¸Šåˆ›å»ºå’Œè¿è¡Œçš„ï¼Œè¯¥å®ä¾‹åŒ…å« 1 ä¸ª NVIDIA A10Gã€‚*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ­å»ºå¼€å‘ç¯å¢ƒ\n",
    "\n",
    "åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ AWS é¢„ç½®çš„ [PyTorch æ·±åº¦å­¦ä¹  AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-pytorch.html)ï¼Œå…¶å·²å®‰è£…äº†æ­£ç¡®çš„ CUDA é©±åŠ¨ç¨‹åºå’Œ PyTorchã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿˜éœ€è¦å®‰è£…ä¸€äº› Hugging Face åº“ï¼ŒåŒ…æ‹¬ transformers å’Œ datasetsã€‚è¿è¡Œä¸‹é¢çš„ä»£ç å°±å¯å®‰è£…æ‰€æœ‰éœ€è¦çš„åŒ…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Hugging Face Libraries\n",
    "!pip install  git+https://github.com/huggingface/peft.git\n",
    "!pip install \"transformers==4.27.1\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes==0.37.1\" loralib --upgrade --quiet\n",
    "# install additional dependencies needed for training\n",
    "!pip install rouge-score tensorboard py7zr "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†\n",
    "\n",
    "è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ [samsum](https://huggingface.co/datasets/samsum) æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤§çº¦ 16k ä¸ªå«æ‘˜è¦çš„èŠå¤©ç±»å¯¹è¯æ•°æ®ã€‚è¿™äº›å¯¹è¯ç”±ç²¾é€šè‹±è¯­çš„è¯­è¨€å­¦å®¶åˆ¶ä½œã€‚\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"id\": \"13818513\",\n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "}\n",
    "```\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨ ğŸ¤— Datasets åº“ä¸­çš„ *â€‹`load_dataset()`* æ–¹æ³•æ¥åŠ è½½ `samsum` æ•°æ®é›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"samsum\")\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "\n",
    "# Train dataset size: 14732\n",
    "# Test dataset size: 819"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬è¦ç”¨ ğŸ¤— Transformers Tokenizer å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºè¯å…ƒ IDã€‚å¦‚æœä½ éœ€è¦äº†è§£è¿™ä¸€æ–¹é¢çš„çŸ¥è¯†ï¼Œè¯·ç§»æ­¥ Hugging Face è¯¾ç¨‹çš„ **[ç¬¬ 6 ç« ](https://huggingface.co/course/chapter6/1?fw=tf)**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id=\"google/flan-t5-xxl\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-XL\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚ç”Ÿæˆå¼æ–‡æœ¬æ‘˜è¦å±äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬å°†æ–‡æœ¬è¾“å…¥ç»™æ¨¡å‹ï¼Œæ¨¡å‹ä¼šè¾“å‡ºæ‘˜è¦ã€‚æˆ‘ä»¬éœ€è¦äº†è§£è¾“å…¥å’Œè¾“å‡ºæ–‡æœ¬çš„é•¿åº¦ä¿¡æ¯ï¼Œä»¥åˆ©äºæˆ‘ä»¬é«˜æ•ˆåœ°æ‰¹é‡å¤„ç†è¿™äº›æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å°†åœ¨è®­ç»ƒå‰ç»Ÿä¸€å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†å¹¶å°†é¢„å¤„ç†åçš„æ•°æ®é›†ä¿å­˜åˆ°ç£ç›˜ã€‚ä½ å¯ä»¥åœ¨æœ¬åœ°æœºå™¨æˆ– CPU ä¸Šè¿è¡Œæ­¤æ­¥éª¤å¹¶å°†å…¶ä¸Šä¼ åˆ° [Hugging Face Hub](https://huggingface.co/docs/hub/datasets-overview)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"data/eval\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä½¿ç”¨ LoRA å’Œ bnb int-8 å¾®è°ƒ T5\n",
    "\n",
    "é™¤äº† LoRA æŠ€æœ¯ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨ [bitsanbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration) æŠŠå†»ç»“çš„ LLM é‡åŒ–ä¸º int8ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°† FLAN-T5 XXL æ‰€éœ€çš„å†…å­˜é™ä½åˆ°çº¦å››åˆ†ä¹‹ä¸€ã€‚\n",
    "\n",
    "è®­ç»ƒçš„ç¬¬ä¸€æ­¥æ˜¯åŠ è½½æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ [philschmid/flan-t5-xxl-sharded-fp16](https://huggingface.co/philschmid/flan-t5-xxl-sharded-fp16) æ¨¡å‹ï¼Œå®ƒæ˜¯ [google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl) çš„åˆ†ç‰‡ç‰ˆã€‚åˆ†ç‰‡å¯ä»¥è®©æˆ‘ä»¬åœ¨åŠ è½½æ¨¡å‹æ—¶ä¸è€—å°½å†…å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `peft` ä¸º LoRA int-8 è®­ç»ƒä½œå‡†å¤‡äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config \n",
    "lora_config = LoraConfig(\n",
    " r=16, \n",
    " lora_alpha=32,\n",
    " target_modules=[\"q\", \"v\"],\n",
    " lora_dropout=0.05,\n",
    " bias=\"none\",\n",
    " task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# trainable params: 18874368 || all params: 11154206720 || trainable%: 0.16921300163961817"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚ä½ æ‰€è§ï¼Œè¿™é‡Œæˆ‘ä»¬åªè®­ç»ƒäº†æ¨¡å‹å‚æ•°çš„ 0.16%ï¼è¿™ä¸ªå·¨å¤§çš„å†…å­˜å¢ç›Šè®©æˆ‘ä»¬å®‰å¿ƒåœ°å¾®è°ƒæ¨¡å‹ï¼Œè€Œä¸ç”¨æ‹…å¿ƒå†…å­˜é—®é¢˜ã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥éœ€è¦åˆ›å»ºä¸€ä¸ª `DataCollatâ€‹â€‹or`ï¼Œè´Ÿè´£å¯¹è¾“å…¥å’Œæ ‡ç­¾è¿›è¡Œå¡«å……ï¼Œæˆ‘ä»¬ä½¿ç”¨ ğŸ¤— Transformers åº“ä¸­çš„`DataCollatâ€‹â€‹orForSeq2Seq` æ¥å®Œæˆè¿™ä¸€ç¯èŠ‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åä¸€æ­¥æ˜¯å®šä¹‰è®­ç»ƒè¶…å‚ (`TrainingArguments`)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir=\"lora-flan-t5-xxl\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\t\tauto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œå¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œå¯¹äº T5ï¼Œå‡ºäºæ”¶æ•›ç¨³å®šæ€§è€ƒé‡ï¼ŒæŸäº›å±‚æˆ‘ä»¬ä»ä¿æŒ `float32` ç²¾åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®­ç»ƒè€—æ—¶çº¦ 10 å°æ—¶ 36 åˆ†é’Ÿï¼Œè®­ç»ƒ 10 å°æ—¶çš„æˆæœ¬çº¦ä¸º `13.22 ç¾å…ƒ`ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¦‚æœ[åœ¨ FLAN-T5-XXL ä¸Šè¿›è¡Œå…¨æ¨¡å‹å¾®è°ƒ](https://www.philschmid.de/fine-tune-flan-t5-deepspeed#3-results--experiments) 10 ä¸ªå°æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ 8 ä¸ª A100 40GBï¼Œæˆæœ¬çº¦ä¸º 322 ç¾å…ƒã€‚\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹ä¿å­˜ä¸‹æ¥ä»¥ç”¨äºåé¢çš„æ¨ç†å’Œè¯„ä¼°ã€‚æˆ‘ä»¬æš‚æ—¶å°†å…¶ä¿å­˜åˆ°ç£ç›˜ï¼Œä½†ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ `model.push_to_hub` æ–¹æ³•å°†å…¶ä¸Šä¼ åˆ° [Hugging Face Hub](https://huggingface.co/docs/hub/main)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our LoRA model & tokenizer results\n",
    "peft_model_id=\"results\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n",
    "# if you want to save the base model to call\n",
    "# trainer.model.base_model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åç”Ÿæˆçš„ LoRA checkpoint æ–‡ä»¶å¾ˆå°ï¼Œä»…éœ€ 84MB å°±åŒ…å«äº†ä» `samsum` æ•°æ®é›†ä¸Šå­¦åˆ°çš„æ‰€æœ‰çŸ¥è¯†ã€‚\n",
    "\n",
    "## 4. ä½¿ç”¨ LoRA FLAN-T5 è¿›è¡Œè¯„ä¼°å’Œæ¨ç†\n",
    "\n",
    "æˆ‘ä»¬å°†ä½¿ç”¨ `evaluate` åº“æ¥è¯„ä¼° `rogue` åˆ†æ•°ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `PEFT` å’Œ `transformers` æ¥å¯¹ FLAN-T5 XXL æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚å¯¹ FLAN-T5 XXL æ¨¡å‹ï¼Œæˆ‘ä»¬è‡³å°‘éœ€è¦ 18GB çš„â€‹â€‹ GPU æ˜¾å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc. \n",
    "peft_model_id = \"results\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ç”¨æµ‹è¯•æ•°æ®é›†ä¸­çš„ä¸€ä¸ªéšæœºæ ·æœ¬æ¥è¯•è¯•æ‘˜è¦æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# Load dataset from the hub and get a sample\n",
    "dataset = load_dataset(\"samsum\")\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "\n",
    "input_ids = tokenizer(sample[\"dialogue\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=True, top_p=0.9)\n",
    "print(f\"input sentence: {sample['dialogue']}\\n{'---'* 20}\")\n",
    "\n",
    "print(f\"summary:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸é”™ï¼æˆ‘ä»¬çš„æ¨¡å‹æœ‰æ•ˆï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹ï¼Œå¹¶ä½¿ç”¨ `test` é›†ä¸­çš„å…¨éƒ¨æ•°æ®å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å®ç°ä¸€äº›å·¥å…·å‡½æ•°æ¥å¸®åŠ©ç”Ÿæˆæ‘˜è¦å¹¶å°†å…¶ä¸ç›¸åº”çš„å‚è€ƒæ‘˜è¦ç»„åˆåˆ°ä¸€èµ·ã€‚è¯„ä¼°æ‘˜è¦ä»»åŠ¡æœ€å¸¸ç”¨çš„æŒ‡æ ‡æ˜¯ [rogue_score](https://en.wikipedia.org/wiki/ROUGE_(metric))ï¼Œå®ƒçš„å…¨ç§°æ˜¯ Recall-Oriented Understudy for Gisting Evaluationã€‚ä¸å¸¸ç”¨çš„å‡†ç¡®ç‡æŒ‡æ ‡ä¸åŒï¼Œå®ƒå°†ç”Ÿæˆçš„æ‘˜è¦ä¸ä¸€ç»„å‚è€ƒæ‘˜è¦è¿›è¡Œæ¯”è¾ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def evaluate_peft_model(sample,max_target_length=50):\n",
    "    # generate summary\n",
    "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)    \n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    # decode eval sample\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
    "    labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    return prediction, labels\n",
    "\n",
    "# load test dataset from distk\n",
    "test_dataset = load_from_disk(\"data/eval/\").with_format(\"torch\")\n",
    "\n",
    "# run predictions\n",
    "# this can take ~45 minutes\n",
    "predictions, references = [] , []\n",
    "for sample in tqdm(test_dataset):\n",
    "    p,l = evaluate_peft_model(sample)\n",
    "    predictions.append(p)\n",
    "    references.append(l)\n",
    "\n",
    "# compute metric \n",
    "rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "# print results \n",
    "print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n",
    "print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n",
    "print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n",
    "print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")\n",
    "\n",
    "# Rogue1: 50.386161%\n",
    "# rouge2: 24.842412%\n",
    "# rougeL: 41.370130%\n",
    "# rougeLsum: 41.394230%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ PEFT å¾®è°ƒåçš„ â€‹â€‹FLAN-T5-XXL åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº† `50.38%` çš„ rogue1 åˆ†æ•°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ[flan-t5-base çš„å…¨æ¨¡å‹å¾®è°ƒè·å¾—äº† 47.23 çš„ rouge1 åˆ†æ•°](https://www.philschmid.de/fine-tune-flan-t5)ã€‚rouge1 åˆ†æ•°æé«˜äº† `3%` ã€‚\n",
    "\n",
    "ä»¤äººéš¾ä»¥ç½®ä¿¡çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ LoRA checkpoint åªæœ‰ 84MBï¼Œè€Œä¸”æ€§èƒ½æ¯”å¯¹æ›´å°çš„æ¨¡å‹è¿›è¡Œå…¨æ¨¡å‹å¾®è°ƒåçš„ checkpoint æ›´å¥½ã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> è‹±æ–‡åŸæ–‡: <url> https://www.philschmid.de/fine-tune-flan-t5-peft </url>\n",
    "\n",
    "> åŸæ–‡ä½œè€…ï¼šPhilipp Schmid\n",
    "\n",
    "> è¯‘è€…: Matrix Yao (å§šä¼Ÿå³°)ï¼Œè‹±ç‰¹å°”æ·±åº¦å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œå·¥ä½œæ–¹å‘ä¸º transformer-family æ¨¡å‹åœ¨å„æ¨¡æ€æ•°æ®ä¸Šçš„åº”ç”¨åŠå¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒæ¨ç†ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
