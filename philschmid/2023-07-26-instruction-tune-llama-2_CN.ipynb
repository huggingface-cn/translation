{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ‰©å±•è¯´æ˜Žï¼šæŒ‡ä»¤å¾®è°ƒLlama 2\n",
    "\n",
    "è¿™ç¯‡åšå®¢æ˜¯ä¸€ç¯‡æ¥è‡ªMeta AIï¼Œå…³äºŽæŒ‡ä»¤å¾®è°ƒLlama 2çš„æ‰©å±•è¯´æ˜Žã€‚æ—¨åœ¨èšç„¦æž„å»ºæŒ‡ä»¤æ•°æ®é›†ï¼Œæœ‰äº†å®ƒï¼Œæˆ‘ä»¬åˆ™å¯ä»¥ä½¿ç”¨è‡ªå·±çš„æŒ‡ä»¤æ¥å¾®è°ƒLlama 2åŸºç¡€æ¨¡åž‹ã€‚\n",
    "\n",
    "ç›®æ ‡æ˜¯æž„å»ºä¸€ä¸ªèƒ½å¤ŸåŸºäºŽè¾“å…¥å†…å®¹æ¥ç”ŸæˆæŒ‡ä»¤çš„æ¨¡åž‹ã€‚è¿™ä¹ˆåšèƒŒåŽçš„é€»è¾‘æ˜¯ï¼Œæ¨¡åž‹å¦‚æ­¤å°±å¯ä»¥ç”±å…¶ä»–äººç”Ÿæˆè‡ªå·±çš„æŒ‡ä»¤æ•°æ®é›†ã€‚è¿™åœ¨å½“æƒ³å¼€å‘ç§äººä¸ªæ€§åŒ–å®šåˆ¶æ¨¡åž‹ï¼Œå¦‚å‘é€æŽ¨ç‰¹ã€å†™é‚®ä»¶ç­‰ï¼Œæ—¶å¾ˆæ–¹ä¾¿ã€‚è¿™ä¹Ÿæ„å‘³ç€ä½ å¯ä»¥é€šè¿‡ä½ çš„é‚®ä»¶æ¥ç”Ÿæˆä¸€ä¸ªæŒ‡ä»¤æ•°æ®é›†ï¼Œç„¶åŽç”¨å®ƒæ¥è®­ç»ƒä¸€ä¸ªæ¨¡åž‹æ¥ä¸ºä½ å†™é‚®ä»¶ã€‚\n",
    "\n",
    "å¥½ï¼Œé‚£æˆ‘ä»¬æ¥å¼€å§‹å§ï¼Ÿæˆ‘ä»¬å°†è¿›è¡Œï¼š\n",
    "\n",
    "1. å®šä¹‰åº”ç”¨åœºæ™¯ç»†èŠ‚å¹¶åˆ›å»ºæŒ‡ä»¤çš„æç¤ºè¯æ¨¡æ¿\n",
    "2. æž„å»ºæŒ‡ä»¤æ•°æ®é›†\n",
    "3. ä½¿ç”¨ `trl` ä¸Ž `SFTTrainer` æŒ‡ä»¤å¾®è°ƒLlama 2\n",
    "4. æµ‹è¯•æ¨¡åž‹ã€è¿›è¡ŒæŽ¨ç†\n",
    "\n",
    "## 1. å®šä¹‰åº”ç”¨åœºæ™¯ç»†èŠ‚å¹¶åˆ›å»ºæŒ‡ä»¤çš„æç¤ºè¯æ¨¡æ¿\n",
    "\n",
    "åœ¨æè¿°åº”ç”¨åœºæ™¯å‰ï¼Œæˆ‘ä»¬è¦æ›´å¥½çš„ç†è§£ä¸€ä¸‹ç©¶ç«Ÿä»€ä¹ˆæ˜¯æŒ‡ä»¤ã€‚\n",
    "\n",
    ">æŒ‡ä»¤æ˜¯ä¸€æ®µæ–‡æœ¬æˆ–æä¾›ç»™å¤§è¯­è¨€æ¨¡åž‹ï¼Œç±»ä¼¼Llamaï¼ŒGPT-4æˆ–Claudeï¼Œä½¿ç”¨çš„æç¤ºè¯ï¼Œç”¨æ¥æŒ‡å¯¼å®ƒåŽ»ç”Ÿæˆå›žå¤ã€‚æŒ‡ä»¤å¯ä»¥è®©äººä»¬åšåˆ°æŠŠæŽ§å¯¹è¯ï¼Œçº¦æŸæ¨¡åž‹è¾“å‡ºæ›´è‡ªç„¶ã€å®žç”¨çš„è¾“å‡ºï¼Œå¹¶ä½¿è¿™äº›ç»“æžœèƒ½å¤Ÿå¯¹é½ç”¨æˆ·çš„ç›®çš„ã€‚åˆ¶ä½œæ¸…æ™°çš„ã€æ•´æ´çš„æŒ‡ä»¤åˆ™æ˜¯ç”Ÿæˆé«˜è´¨é‡å¯¹è¯çš„å…³é”®ã€‚\n",
    ">\n",
    "\n",
    "æŒ‡ä»¤çš„ä¾‹å­å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚\n",
    "\n",
    "| èƒ½åŠ› | ç¤ºä¾‹æŒ‡ä»¤ |\n",
    "| --- | --- |\n",
    "| å¤´è„‘é£Žæš´ | æä¾›ä¸€ç³»åˆ—æ–°å£å‘³çš„å†°æ·‡æ·‹çš„åˆ›æ„ã€‚ |\n",
    "| åˆ†ç±» | æ ¹æ®å‰§æƒ…æ¦‚è¦ï¼Œå°†è¿™äº›ç”µå½±å½’ç±»ä¸ºå–œå‰§ã€æˆå‰§æˆ–ææ€–ç‰‡ã€‚ |\n",
    "| ç¡®å®šæ€§é—®ç­” | ç”¨ä¸€ä¸ªå•è¯å›žç­”â€œæ³•å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿâ€ |\n",
    "| ç”Ÿæˆ | ç”¨ç½—ä¼¯ç‰¹Â·å¼—ç½—æ–¯ç‰¹çš„é£Žæ ¼å†™ä¸€é¦–å…³äºŽå¤§è‡ªç„¶å’Œå­£èŠ‚å˜åŒ–çš„è¯—ã€‚ |\n",
    "| ä¿¡æ¯æå– | ä»Žè¿™ç¯‡çŸ­æ–‡ä¸­æå–ä¸»è¦äººç‰©çš„åå­—ã€‚ |\n",
    "| å¼€æ”¾æ€§é—®ç­” | ä¸ºä»€ä¹ˆæ ‘å¶åœ¨ç§‹å¤©ä¼šå˜è‰²ï¼Ÿç”¨ç§‘å­¦çš„ç†ç”±è§£é‡Šä¸€ä¸‹ã€‚ |\n",
    "| æ‘˜è¦ | ç”¨2-3å¥è¯æ¦‚æ‹¬ä¸€ä¸‹è¿™ç¯‡å…³äºŽå¯å†ç”Ÿèƒ½æºæœ€æ–°è¿›å±•çš„æ–‡ç« ã€‚ |\n",
    "\n",
    "å¦‚å¼€å¤´æ‰€è¿°ï¼Œæˆ‘ä»¬æƒ³è¦å¾®è°ƒæ¨¡åž‹ï¼Œä»¥ä¾¿æ ¹æ®è¾“å…¥ï¼ˆæˆ–è¾“å‡ºï¼‰ç”ŸæˆæŒ‡ä»¤ã€‚ æˆ‘ä»¬å¸Œæœ›å°†å…¶ç”¨ä½œåˆ›å»ºåˆæˆæ•°æ®é›†çš„æ–¹æ³•ï¼Œä»¥èµ‹äºˆLLMå’Œä»£ç†ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚\n",
    "\n",
    "æŠŠè¿™ä¸ªæƒ³æ³•è½¬æ¢æˆä¸€ä¸ªåŸºç¡€çš„æç¤ºæ¨¡æ¿ï¼ŒæŒ‰ç…§[Alpacaæ ¼å¼](https://github.com/tatsu-lab/stanford_alpaca#data-release).\n",
    "\n",
    "\n",
    "```python\n",
    "### æŒ‡ä»¤:\n",
    "ä½¿ç”¨ä¸‹é¢çš„è¾“å…¥åˆ›å»ºä¸€æ¡æŒ‡ä»¤ï¼Œè¯¥æŒ‡ä»¤å¯ä»¥ä½¿ç”¨LLMç”Ÿæˆè¾“å…¥æ–‡æœ¬ã€‚\n",
    "\n",
    "### è¾“å…¥æ–‡æœ¬:\n",
    "äº²çˆ±çš„ [é¢†å¯¼åå­—]ï¼Œ\n",
    "æ­¤å°ä¿¡ä»¶æ˜¯æƒ³ç”³è¯·æˆ‘ä¸‹å‘¨ä¸€åˆ°å‘¨å››çš„å¸¦è–ªå‡æœŸã€‚\n",
    "\n",
    "ä¸‹å‘¨æˆ‘æœ‰ä¸€äº›ç§äººäº‹åŠ¡è¦å¤„ç†ï¼Œä¸å¾—ä¸ç¦»å¼€åŠžå…¬\n",
    "å®¤ã€‚æˆ‘æƒ³å°½æå‰åœ°é€šçŸ¥æ‚¨ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥åœ¨æˆ‘ç¦»\n",
    "å¼€æœŸé—´åšå¥½ç›¸åº”çš„å®‰æŽ’ã€‚\n",
    "\n",
    "å¦‚æžœæ‚¨éœ€è¦ä»»ä½•å…¶ä»–ä¿¡æ¯ï¼Œæˆ–è€…å¯¹æˆ‘ä¸‹å‘¨ä¼‘å‡æœ‰\n",
    "ä»»ä½•ç–‘é—®ï¼Œè¯·è”ç³»æˆ‘ã€‚è°¢è°¢æ‚¨å¯¹æ­¤çš„å—ç†ã€‚\n",
    "\n",
    "è°¢è°¢ï¼Œ[ä½ çš„åå­—]\n",
    "\n",
    "### å›žå¤:\n",
    "ç»™æˆ‘çš„é¢†å¯¼å†™å°é‚®ä»¶ï¼Œè¯´æˆ‘ä¸‹å‘¨8æœˆ1æ—¥åˆ°8æœˆ4æ—¥éœ€è¦è¯·å‡ã€‚\n",
    "```\n",
    "\n",
    "## 2. åˆ›å»ºæŒ‡ä»¤æ•°æ®é›†\n",
    "\n",
    "åœ¨å®šä¹‰äº†æˆ‘ä»¬çš„åº”ç”¨åœºæ™¯å’Œæç¤ºæ¨¡æ¿åŽï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºè‡ªå·±çš„æŒ‡ä»¤æ•°æ®é›†ã€‚åˆ›å»ºé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®é›†æ˜¯èŽ·å¾—è‰¯å¥½æ¨¡åž‹æ€§èƒ½çš„å…³é”®ã€‚ç ”ç©¶è¡¨æ˜Žï¼Œ[â€œå¯¹é½ï¼Œè¶Šå°‘è¶Šå¥½â€](https://arxiv.org/abs/2305.11206)è¡¨æ˜Žï¼Œåˆ›å»ºé«˜è´¨é‡ã€ä½Žæ•°é‡(å¤§çº¦1000ä¸ªæ ·æœ¬)çš„æ•°æ®é›†å¯ä»¥è¾¾åˆ°ä¸Žä½Žè´¨é‡ã€é«˜æ•°é‡çš„æ•°æ®é›†ç›¸åŒçš„æ€§èƒ½ã€‚\n",
    "\n",
    "åˆ›å»ºæŒ‡ä»¤æ•°æ®é›†æœ‰å‡ ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬:\n",
    "\n",
    "1. ä½¿ç”¨çŽ°æœ‰æ•°æ®é›†å¹¶å°†å…¶è½¬æ¢ä¸ºæŒ‡ä»¤æ•°æ®é›†ï¼Œä¾‹å¦‚[FLAN](https://huggingface.co/datasets/SirNeural/flan_v2)\n",
    "2. ä½¿ç”¨çŽ°æœ‰çš„LLMåˆ›å»ºåˆæˆæŒ‡ä»¤æ•°æ®é›†ï¼Œä¾‹å¦‚[Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)\n",
    "3. äººåŠ›åˆ›å»ºæŒ‡ä»¤æ•°æ®é›†ï¼Œä¾‹å¦‚[Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)ã€‚\n",
    "\n",
    "æ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ä¼˜ç¼ºç‚¹ï¼Œè¿™å–å†³äºŽé¢„ç®—ã€æ—¶é—´å’Œè´¨é‡è¦æ±‚ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨çŽ°æœ‰æ•°æ®é›†æ˜¯æœ€ç®€å•çš„ï¼Œä½†å¯èƒ½ä¸é€‚åˆæ‚¨çš„ç‰¹å®šç”¨ä¾‹ï¼Œè€Œä½¿ç”¨äººåŠ›å¯èƒ½æ˜¯æœ€å‡†ç¡®çš„ï¼Œä½†å¿…ç„¶è€—æ—¶ã€æ˜‚è´µã€‚ä¹Ÿå¯ä»¥ç»“åˆå‡ ç§ä¸åŒæ–¹æ³•æ¥åˆ›å»ºæŒ‡ä»¤æ•°æ®é›†ï¼Œå¦‚[Orca: Progressive Learning from Complex Explanation Traces of GPT-4.](https://arxiv.org/abs/2306.02707)ã€‚\n",
    "\n",
    "ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ **[Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)** ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„æŒ‡ä»¤è·Ÿè¸ªè®°å½•æ•°æ®é›†ï¼Œç”±æ•°åƒåDatabrickså‘˜å·¥åœ¨ **[InstructGPT paper](https://arxiv.org/abs/2203.02155)** ä¸­æè¿°çš„å‡ ä¸ªè¡Œä¸ºç±»åˆ«ä¸­ç”Ÿæˆï¼ŒåŒ…æ‹¬å¤´è„‘é£Žæš´ã€åˆ†ç±»ã€ç¡®å®šæ€§å›žç­”ã€ç”Ÿæˆã€ä¿¡æ¯æå–ã€å¼€æ”¾æ€§å›žç­”å’Œæ‘˜è¦ã€‚\n",
    "\n",
    "å¼€å§‹ç¼–ç¨‹å§ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬æ¥å®‰è£…ä¾èµ–é¡¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŠ è½½ **`databricks/databricks-dolly-15k`**Â æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨ ðŸ¤— Datasets libraryçš„ **`load_dataset()`** æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 15011\n",
      "{'instruction': 'On what month and day was Antwan Deon Odom born?', 'context': 'Antwan Deon Odom (born September 24, 1981) is a former American football defensive end. He was drafted by the Tennessee Titans in the second round of the 2004 NFL Draft. He played college football at Alabama. He has also played for the Cincinnati Bengals.', 'response': 'September 24', 'category': 'closed_qa'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†æŒ‡å¯¼æˆ‘ä»¬çš„æ¨¡åž‹ï¼Œæˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬çš„ç»“æž„åŒ–ç¤ºä¾‹è½¬æ¢ä¸ºé€šè¿‡æŒ‡ä»¤æè¿°çš„ä»»åŠ¡é›†åˆã€‚æˆ‘ä»¬å®šä¹‰ä¸€ä¸ª **`formatting_function`** ï¼Œå®ƒæŽ¥å—ä¸€ä¸ªæ ·æœ¬å¹¶è¿”å›žä¸€ä¸ªç¬¦åˆæ ¼å¼æŒ‡ä»¤çš„å­—ç¬¦ä¸²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    "\n",
    "### Input:\n",
    "{sample['response']}\n",
    "\n",
    "### Response:\n",
    "{sample['instruction']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
      "\n",
      "### Input:\n",
      "U.S. cities in the list: Chicago, New York, Atlanta, Honolulu, Austin\n",
      "U.S. states in the list: Montana, New York, Florida, South Dakota\n",
      "\n",
      "### Response:\n",
      "Which are the U.S. cities and which are U.S. states in this list? Montana, Chicago, New York, Atlanta, Honolulu, Florida, Austin, South Dakota\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_instruction(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instruction-tune Llama 2 using `trl` and the `SFTTrainer`\n",
    "\n",
    " We will use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2305.14314)\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is:\n",
    "\n",
    "- Quantize the pre-trained model to 4 bits and freeze it.\n",
    "- Attach small, trainable adapter layers. (LoRA)\n",
    "- Finetune only the adapter layers while using the frozen quantized model for context.\n",
    "\n",
    "If you want to learn more about QLoRA and how it works, I recommend you to read theÂ [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)Â blog post.\n",
    "\n",
    "### Flash Attention\n",
    "\n",
    "Flash Attention is a an method that reorders the attention computation and leverages classical techniques (tiling, recomputation) to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. It is based on the paper \"[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)\".\n",
    "The TL;DR; accelerates training up to 3x. Learn more at [FlashAttention](https://github.com/Dao-AILab/flash-attention/tree/main). Flash Attention is currently only available for Ampere (A10, A40, A100, ...) & Hopper (H100, ...) GPUs. You can check if your GPU is supported and install it using the following command:\n",
    "\n",
    "_Note: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of `MAX_JOBS`. On the `g5.2xlarge` we used `4`._\n",
    "\n",
    "```bash\n",
    "python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\"\n",
    "pip install ninja packaging\n",
    "MAX_JOBS=4 pip install flash-attn --no-build-isolation\n",
    "```\n",
    "\n",
    "_Installing flash attention can take quite a bit of time (10-45 minutes)._\n",
    "\n",
    "The example supports the use of Flash Attention for all Llama checkpoints, but is not enabled by default. To use Flash Attention comment in the code block below wich says  `# COMMENT IN TO USE FLASH ATTENTION`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006969451904296875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b7caee136b47528f5606c5f95c20e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "use_flash_attention = False\n",
    "\n",
    "# COMMENT IN TO USE FLASH ATTENTION\n",
    "# replace attention with flash attention \n",
    "# if torch.cuda.get_device_capability()[0] >= 8:\n",
    "#     from utils.llama_patch import replace_attn_with_flash_attn\n",
    "#     print(\"Using flash attention\")\n",
    "#     replace_attn_with_flash_attn()\n",
    "#     use_flash_attention = True\n",
    "\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # non-gated\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n",
    "\n",
    "\n",
    "# BitsAndBytesConfig int-4 config \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_cache=False, device_map=\"auto\")\n",
    "model.config.pretraining_tp = 1 \n",
    "\n",
    "# Validate that the model is using flash attention, by comparing doc strings\n",
    "if use_flash_attention:\n",
    "    from utils.llama_patch import forward    \n",
    "    assert model.model.layers[0].self_attn.forward.__doc__ == forward.__doc__, \"Model is not using flash attention\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TheÂ `SFTTrainer`Â  supports a native integration withÂ `peft`, which makes it super easy to efficiently instruction tune LLMs. We only need to create ourÂ `LoRAConfig`Â and provide it to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA config based on QLoRA paper\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")\n",
    "\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start our training we need to define the hyperparameters (`TrainingArguments`) we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llama-7-int4-dolly\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=6 if use_flash_attention else 4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=True # disable tqdm since with packing values are in correct\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have every building block we need to create ourÂ `SFTTrainer`Â to start then training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048 # max sequence length for model and packing of the dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction, \n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training our model by calling the `train()` method on our `Trainer` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer.train() # there will not be a progress bar since tqdm is disabled\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training without Flash Attention enabled took 03:08:00 on a `g5.2xlarge`. The instance costs `1,212$/h` which brings us to a total cost of `3.7$`. \n",
    "The training with Flash Attention enabled took 02:08:00 on a `g5.2xlarge`. The instance costs `1,212$/h` which brings us to a total cost of `2.6$`.\n",
    "\n",
    "The results using Flash Attention are mind blowing and impressive, 1.5x faster and 30% cheaper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Model and run Inference\n",
    "\n",
    "After the training is done we want to run and test our model. We will use `peft` and `transformers` to load our LoRA adapter into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_flash_attention:\n",
    "    # unpatch flash attention\n",
    "    from utils.llama_patch import unplace_flash_attn_with_attn\n",
    "    unplace_flash_attn_with_attn()\n",
    "    \n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "args.output_dir = \"llama-7-int4-dolly\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ") \n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s load the dataset again with a random sample to try to generate an instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# Load dataset from the hub and get a sample\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "sample = dataset[randrange(len(dataset))]\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    "\n",
    "### Input:\n",
    "{sample['response']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "print(f\"Prompt:\\n{sample['response']}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"Ground truth:\\n{sample['instruction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! our model works! If want to accelerate our model we can deploy it with [Text Generation Inference](https://github.com/huggingface/text-generation-inference). Therefore we would need to merge our adapter weights into the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    ") \n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "\n",
    "# push merged model to the hub\n",
    "# merged_model.push_to_hub(\"user/repo\")\n",
    "# tokenizer.push_to_hub(\"user/repo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
