{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ‰©å±•è¯´æ˜: æŒ‡ä»¤å¾®è°ƒ Llama 2\n",
    "\n",
    "è¿™ç¯‡åšå®¢æ˜¯ä¸€ç¯‡æ¥è‡ª Meta AIï¼Œå…³äºæŒ‡ä»¤å¾®è°ƒ Llama 2 çš„æ‰©å±•è¯´æ˜ã€‚æ—¨åœ¨èšç„¦æ„å»ºæŒ‡ä»¤æ•°æ®é›†ï¼Œæœ‰äº†å®ƒï¼Œæˆ‘ä»¬åˆ™å¯ä»¥ä½¿ç”¨è‡ªå·±çš„æŒ‡ä»¤æ¥å¾®è°ƒ Llama 2 åŸºç¡€æ¨¡å‹ã€‚\n",
    "\n",
    "ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªèƒ½å¤ŸåŸºäºè¾“å…¥å†…å®¹æ¥ç”ŸæˆæŒ‡ä»¤çš„æ¨¡å‹ã€‚è¿™ä¹ˆåšèƒŒåçš„é€»è¾‘æ˜¯ï¼Œæ¨¡å‹å¦‚æ­¤å°±å¯ä»¥ç”±å…¶ä»–äººç”Ÿæˆè‡ªå·±çš„æŒ‡ä»¤æ•°æ®é›†ã€‚è¿™åœ¨å½“æƒ³å¼€å‘ç§äººä¸ªæ€§åŒ–å®šåˆ¶æ¨¡å‹ï¼Œå¦‚å‘é€æ¨ç‰¹ã€å†™é‚®ä»¶ç­‰ï¼Œæ—¶å¾ˆæ–¹ä¾¿ã€‚è¿™ä¹Ÿæ„å‘³ç€ä½ å¯ä»¥é€šè¿‡ä½ çš„é‚®ä»¶æ¥ç”Ÿæˆä¸€ä¸ªæŒ‡ä»¤æ•°æ®é›†ï¼Œç„¶åç”¨å®ƒæ¥è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥ä¸ºä½ å†™é‚®ä»¶ã€‚\n",
    "\n",
    "å¥½ï¼Œé‚£æˆ‘ä»¬æ¥å¼€å§‹å§ï¼Ÿæˆ‘ä»¬å°†è¿›è¡Œ:\n",
    "\n",
    "1. å®šä¹‰åº”ç”¨åœºæ™¯ç»†èŠ‚å¹¶åˆ›å»ºæŒ‡ä»¤çš„æç¤ºè¯æ¨¡æ¿\n",
    "2. æ„å»ºæŒ‡ä»¤æ•°æ®é›†\n",
    "3. ä½¿ç”¨ `trl` ä¸ `SFTTrainer` æŒ‡ä»¤å¾®è°ƒ Llama 2\n",
    "4. æµ‹è¯•æ¨¡å‹ã€è¿›è¡Œæ¨ç†\n",
    "\n",
    "## 1. å®šä¹‰åº”ç”¨åœºæ™¯ç»†èŠ‚å¹¶åˆ›å»ºæŒ‡ä»¤çš„æç¤ºè¯æ¨¡æ¿\n",
    "\n",
    "åœ¨æè¿°åº”ç”¨åœºæ™¯å‰ï¼Œæˆ‘ä»¬è¦æ›´å¥½çš„ç†è§£ä¸€ä¸‹ç©¶ç«Ÿä»€ä¹ˆæ˜¯æŒ‡ä»¤ã€‚\n",
    "\n",
    "> æŒ‡ä»¤æ˜¯ä¸€æ®µæ–‡æœ¬æˆ–æä¾›ç»™å¤§è¯­è¨€æ¨¡å‹ï¼Œç±»ä¼¼ Llamaï¼ŒGPT-4 æˆ– Claudeï¼Œä½¿ç”¨çš„æç¤ºè¯ï¼Œç”¨æ¥æŒ‡å¯¼å®ƒå»ç”Ÿæˆå›å¤ã€‚æŒ‡ä»¤å¯ä»¥è®©äººä»¬åšåˆ°æŠŠæ§å¯¹è¯ï¼Œçº¦æŸæ¨¡å‹è¾“å‡ºæ›´è‡ªç„¶ã€å®ç”¨çš„è¾“å‡ºï¼Œå¹¶ä½¿è¿™äº›ç»“æœèƒ½å¤Ÿå¯¹é½ç”¨æˆ·çš„ç›®çš„ã€‚åˆ¶ä½œæ¸…æ™°çš„ã€æ•´æ´çš„æŒ‡ä»¤åˆ™æ˜¯ç”Ÿæˆé«˜è´¨é‡å¯¹è¯çš„å…³é”®ã€‚\n",
    "\n",
    "æŒ‡ä»¤çš„ä¾‹å­å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚\n",
    "\n",
    "| èƒ½åŠ› | ç¤ºä¾‹æŒ‡ä»¤ |\n",
    "| --- | --- |\n",
    "| å¤´è„‘é£æš´ | æä¾›ä¸€ç³»åˆ—æ–°å£å‘³çš„å†°æ·‡æ·‹çš„åˆ›æ„ã€‚ |\n",
    "| åˆ†ç±» | æ ¹æ®å‰§æƒ…æ¦‚è¦ï¼Œå°†è¿™äº›ç”µå½±å½’ç±»ä¸ºå–œå‰§ã€æˆå‰§æˆ–ææ€–ç‰‡ã€‚ |\n",
    "| ç¡®å®šæ€§é—®ç­” | ç”¨ä¸€ä¸ªå•è¯å›ç­”â€œæ³•å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿâ€ |\n",
    "| ç”Ÿæˆ | ç”¨ç½—ä¼¯ç‰¹Â·å¼—ç½—æ–¯ç‰¹çš„é£æ ¼å†™ä¸€é¦–å…³äºå¤§è‡ªç„¶å’Œå­£èŠ‚å˜åŒ–çš„è¯—ã€‚ |\n",
    "| ä¿¡æ¯æå– | ä»è¿™ç¯‡çŸ­æ–‡ä¸­æå–ä¸»è¦äººç‰©çš„åå­—ã€‚ |\n",
    "| å¼€æ”¾æ€§é—®ç­” | ä¸ºä»€ä¹ˆæ ‘å¶åœ¨ç§‹å¤©ä¼šå˜è‰²ï¼Ÿç”¨ç§‘å­¦çš„ç†ç”±è§£é‡Šä¸€ä¸‹ã€‚ |\n",
    "| æ‘˜è¦ | ç”¨ 2-3 å¥è¯æ¦‚æ‹¬ä¸€ä¸‹è¿™ç¯‡å…³äºå¯å†ç”Ÿèƒ½æºæœ€æ–°è¿›å±•çš„æ–‡ç« ã€‚ |\n",
    "\n",
    "å¦‚å¼€å¤´æ‰€è¿°ï¼Œæˆ‘ä»¬æƒ³è¦å¾®è°ƒæ¨¡å‹ï¼Œä»¥ä¾¿æ ¹æ®è¾“å…¥ (æˆ–è¾“å‡º) ç”ŸæˆæŒ‡ä»¤ã€‚ æˆ‘ä»¬å¸Œæœ›å°†å…¶ç”¨ä½œåˆ›å»ºåˆæˆæ•°æ®é›†çš„æ–¹æ³•ï¼Œä»¥èµ‹äºˆ LLM å’Œä»£ç†ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚\n",
    "\n",
    "æŠŠè¿™ä¸ªæƒ³æ³•è½¬æ¢æˆä¸€ä¸ªåŸºç¡€çš„æç¤ºæ¨¡æ¿ï¼ŒæŒ‰ç…§ [Alpaca æ ¼å¼](https://github.com/tatsu-lab/stanford_alpaca#data-release).\n",
    "\n",
    "```python\n",
    "### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    "\n",
    "### Input:\n",
    "Dear [boss name],\n",
    "\n",
    "I'm writing to request next week, August 1st through August 4th,\n",
    "off as paid time off.\n",
    "\n",
    "I have some personal matters to attend to that week that require \n",
    "me to be out of the office. I wanted to give you as much advance \n",
    "notice as possible so you can plan accordingly while I am away.\n",
    "\n",
    "Please let me know if you need any additional information from me \n",
    "or have any concerns with me taking next week off. I appreciate you \n",
    "considering this request.\n",
    "\n",
    "Thank you, [Your name]\n",
    "\n",
    "### Response:\n",
    "Write an email to my boss that I need next week 08/01 - 08/04 off.\n",
    "```\n",
    "\n",
    "## 2. åˆ›å»ºæŒ‡ä»¤æ•°æ®é›†\n",
    "\n",
    "åœ¨å®šä¹‰äº†æˆ‘ä»¬çš„åº”ç”¨åœºæ™¯å’Œæç¤ºæ¨¡æ¿åï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºè‡ªå·±çš„æŒ‡ä»¤æ•°æ®é›†ã€‚åˆ›å»ºé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®é›†æ˜¯è·å¾—è‰¯å¥½æ¨¡å‹æ€§èƒ½çš„å…³é”®ã€‚ç ”ç©¶è¡¨æ˜ï¼Œ[â€œå¯¹é½ï¼Œè¶Šå°‘è¶Šå¥½â€](https://arxiv.org/abs/2305.11206) è¡¨æ˜ï¼Œåˆ›å»ºé«˜è´¨é‡ã€ä½æ•°é‡ (å¤§çº¦ 1000 ä¸ªæ ·æœ¬) çš„æ•°æ®é›†å¯ä»¥è¾¾åˆ°ä¸ä½è´¨é‡ã€é«˜æ•°é‡çš„æ•°æ®é›†ç›¸åŒçš„æ€§èƒ½ã€‚\n",
    "\n",
    "åˆ›å»ºæŒ‡ä»¤æ•°æ®é›†æœ‰å‡ ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬:\n",
    "\n",
    "1. ä½¿ç”¨ç°æœ‰æ•°æ®é›†å¹¶å°†å…¶è½¬æ¢ä¸ºæŒ‡ä»¤æ•°æ®é›†ï¼Œä¾‹å¦‚ [FLAN](https://huggingface.co/datasets/SirNeural/flan_v2)\n",
    "2. ä½¿ç”¨ç°æœ‰çš„ LLM åˆ›å»ºåˆæˆæŒ‡ä»¤æ•°æ®é›†ï¼Œä¾‹å¦‚ [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)\n",
    "3. äººåŠ›åˆ›å»ºæŒ‡ä»¤æ•°æ®é›†ï¼Œä¾‹å¦‚ [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)ã€‚\n",
    "\n",
    "æ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ä¼˜ç¼ºç‚¹ï¼Œè¿™å–å†³äºé¢„ç®—ã€æ—¶é—´å’Œè´¨é‡è¦æ±‚ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ç°æœ‰æ•°æ®é›†æ˜¯æœ€ç®€å•çš„ï¼Œä½†å¯èƒ½ä¸é€‚åˆæ‚¨çš„ç‰¹å®šç”¨ä¾‹ï¼Œè€Œä½¿ç”¨äººåŠ›å¯èƒ½æ˜¯æœ€å‡†ç¡®çš„ï¼Œä½†å¿…ç„¶è€—æ—¶ã€æ˜‚è´µã€‚ä¹Ÿå¯ä»¥ç»“åˆå‡ ç§ä¸åŒæ–¹æ³•æ¥åˆ›å»ºæŒ‡ä»¤æ•°æ®é›†ï¼Œå¦‚ [Orca: Progressive Learning from Complex Explanation Traces of GPT-4.](https://arxiv.org/abs/2306.02707)ã€‚\n",
    "\n",
    "ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ **[Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)**ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„æŒ‡ä»¤è·Ÿè¸ªè®°å½•æ•°æ®é›†ï¼Œç”±æ•°åƒå Databricks å‘˜å·¥åœ¨ **[InstructGPT paper](https://arxiv.org/abs/2203.02155)** ä¸­æè¿°çš„å‡ ä¸ªè¡Œä¸ºç±»åˆ«ä¸­ç”Ÿæˆï¼ŒåŒ…æ‹¬å¤´è„‘é£æš´ã€åˆ†ç±»ã€ç¡®å®šæ€§å›ç­”ã€ç”Ÿæˆã€ä¿¡æ¯æå–ã€å¼€æ”¾æ€§å›ç­”å’Œæ‘˜è¦ã€‚\n",
    "\n",
    "å¼€å§‹ç¼–ç¨‹å§ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬æ¥å®‰è£…ä¾èµ–é¡¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ä½¿ç”¨ ğŸ¤— Datasets library çš„ **`load_dataset()`** æ–¹æ³•åŠ è½½ **`databricks/databricks-dolly-15k`**Â æ•°æ®é›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# ä»hubåŠ è½½æ•°æ®é›†\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†æŒ‡å¯¼æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬çš„ç»“æ„åŒ–ç¤ºä¾‹è½¬æ¢ä¸ºé€šè¿‡æŒ‡ä»¤æè¿°çš„ä»»åŠ¡é›†åˆã€‚æˆ‘ä»¬å®šä¹‰ä¸€ä¸ª **`formatting_function`** ï¼Œå®ƒæ¥å—ä¸€ä¸ªæ ·æœ¬å¹¶è¿”å›ä¸€ä¸ªç¬¦åˆæ ¼å¼æŒ‡ä»¤çš„å­—ç¬¦ä¸²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    "\n",
    "### Input:\n",
    "{sample['response']}\n",
    "\n",
    "### Response:\n",
    "{sample['instruction']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æ¥åœ¨ä¸€ä¸ªéšæœºçš„ä¾‹å­ä¸Šæµ‹è¯•ä¸€ä¸‹æˆ‘ä»¬çš„ç»“æ„åŒ–å‡½æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_instruction(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä½¿ç”¨ `trl` å’Œ`SFTTrainer` æŒ‡ä»¤å¾®è°ƒ Llama 2\n",
    "\n",
    "æˆ‘ä»¬å°†ä½¿ç”¨æœ€è¿‘åœ¨ç”± Tim Dettmers ç­‰äººçš„å‘è¡¨çš„è®ºæ–‡â€œ[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2305.14314)â€ä¸­ä»‹ç»çš„æ–¹æ³•ã€‚QLoRA æ˜¯ä¸€ç§æ–°çš„æŠ€æœ¯ï¼Œç”¨äºåœ¨å¾®è°ƒæœŸé—´å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…å­˜å ç”¨ï¼Œä¸”å¹¶ä¸ä¼šé™ä½æ€§èƒ½ã€‚QLoRA çš„ TL;DR; æ˜¯è¿™æ ·å·¥ä½œçš„:\n",
    "\n",
    "- å°†é¢„è®­ç»ƒæ¨¡å‹é‡åŒ–ä¸º 4bit ä½å¹¶å†»ç»“å®ƒã€‚\n",
    "- é™„åŠ è½»é‡åŒ–çš„ã€å¯è®­ç»ƒçš„é€‚é…å™¨å±‚ã€‚(LoRA)\n",
    "- åœ¨ä½¿ç”¨å†»ç»“çš„é‡åŒ–æ¨¡å‹åŸºäºæ–‡æœ¬å†…å®¹è¿›è¡Œå¾®è°ƒæ—¶ï¼Œä»…å¾®è°ƒé€‚é…å™¨å±‚å‚æ•°ã€‚\n",
    "\n",
    "å¦‚æœæ‚¨æƒ³äº†è§£æœ‰å…³ QLoRA åŠå…¶å·¥ä½œåŸç†çš„æ›´å¤šä¿¡æ¯ï¼Œæˆ‘å»ºè®®æ‚¨é˜…è¯» **[Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)** åšå®¢æ–‡ç« ã€‚\n",
    "\n",
    "### Flash Attention (å¿«é€Ÿæ³¨æ„åŠ›)\n",
    "\n",
    "Flash Attention æ˜¯ä¸€ç§ç»è¿‡é‡æ–°æ’åºçš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ç»å…¸æŠ€æœ¯ (æ’åˆ—ã€é‡è®¡ç®—) æ¥æ˜¾è‘—åŠ å¿«é€Ÿåº¦ï¼Œå°†åºåˆ—é•¿åº¦çš„å†…å­˜ä½¿ç”¨é‡ä»äºŒæ¬¡é™ä½åˆ°çº¿æ€§ã€‚å®ƒåŸºäºè®ºæ–‡â€œ[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)â€ã€‚\n",
    "\n",
    "TL;DR; å°†è®­ç»ƒåŠ é€Ÿäº† 3 å€ã€‚åœ¨è¿™å„¿è·å¾—æ›´å¤šä¿¡æ¯ [FlashAttention](https://github.com/Dao-AILab/flash-attention/tree/main)ã€‚ Flash Attention ç›®å‰ä»…æ”¯æŒ Ampere (A10, A40, A100, â€¦) & Hopper (H100, â€¦) GPUã€‚ ä½ å¯ä»¥æ£€æŸ¥ä¸€ä¸‹ä½ çš„ GPU æ˜¯å¦æ”¯æŒï¼Œå¹¶ç”¨ä¸‹é¢çš„å‘½ä»¤æ¥å®‰è£…å®ƒ:\n",
    "\n",
    "æ³¨æ„: å¦‚æœæ‚¨çš„æœºå™¨çš„å†…å­˜å°äº 96GBï¼Œè€Œ CPU æ ¸å¿ƒæ•°è¶³å¤Ÿå¤šï¼Œè¯·å‡å°‘ `MAX_JOBS` çš„æ•°é‡ã€‚åœ¨æˆ‘ä»¬ä½¿ç”¨çš„ `g5.2xlarge` ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `4` ã€‚\n",
    "\n",
    "```bash\n",
    "python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\"\n",
    "pip install ninja packaging\n",
    "MAX_JOBS=4 pip install flash-attn --no-build-isolation\n",
    "```\n",
    "\n",
    "_å®‰è£… flash attention æ˜¯ä¼šéœ€è¦ä¸€äº›æ—¶é—´ (10-45 åˆ†é’Ÿ)_ã€‚\n",
    "\n",
    "è¯¥ç¤ºä¾‹æ”¯æŒå¯¹æ‰€æœ‰ Llama æ£€æŸ¥ç‚¹ä½¿ç”¨ Flash Attentionï¼Œä½†é»˜è®¤æ˜¯æœªå¯ç”¨çš„ã€‚è¦å¼€å¯ Flash Attentionï¼Œè¯·å–æ¶ˆä»£ç å—ä¸­è¿™æ®µçš„æ³¨é‡Šï¼Œ `# COMMENT IN TO USE FLASH ATTENTION` ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "use_flash_attention = False\n",
    "\n",
    "# COMMENT IN TO USE FLASH ATTENTION\n",
    "# replace attention with flash attention \n",
    "# if torch.cuda.get_device_capability()[0] >= 8:\n",
    "#     from utils.llama_patch import replace_attn_with_flash_attn\n",
    "#     print(\"Using flash attention\")\n",
    "#     replace_attn_with_flash_attn()\n",
    "#     use_flash_attention = True\n",
    "\n",
    "\n",
    "# Hugging Face æ¨¡å‹id\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # non-gated\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n",
    "\n",
    "\n",
    "# BitsAndBytesConfig int-4 config \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_cache=False, device_map=\"auto\")\n",
    "model.config.pretraining_tp = 1 \n",
    "\n",
    "# é€šè¿‡å¯¹æ¯”docä¸­çš„å­—ç¬¦ä¸²ï¼ŒéªŒè¯æ¨¡å‹æ˜¯åœ¨ä½¿ç”¨flash attention\n",
    "if use_flash_attention:\n",
    "    from utils.llama_patch import forward    \n",
    "    assert model.model.layers[0].self_attn.forward.__doc__ == forward.__doc__, \"Model is not using flash attention\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`SFTTrainer`** æ”¯æŒä¸ **`peft`** çš„æœ¬åœ°é›†æˆï¼Œè¿™ä½¿å¾—é«˜æ•ˆåœ°æŒ‡ä»¤å¾®è°ƒLLMå˜å¾—éå¸¸å®¹æ˜“ã€‚æˆ‘ä»¬åªéœ€è¦åˆ›å»º **`LoRAConfig`** å¹¶å°†å…¶æä¾›ç»™è®­ç»ƒå™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# åŸºäº QLoRA è®ºæ–‡æ¥é…ç½® LoRA\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")\n",
    "\n",
    "\n",
    "# ä¸ºè®­ç»ƒå‡†å¤‡å¥½æ¨¡å‹\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰è‡ªå·±æƒ³è¦çš„è¶…å‚æ•° (`TrainingArguments`)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llama-7-int4-dolly\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=6 if use_flash_attention else 4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=True # å½“é…ç½®çš„å‚æ•°éƒ½æ­£ç¡®åå¯ä»¥å…³é—­tqdm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ç°åœ¨æœ‰äº†ç”¨æ¥è®­ç»ƒæ¨¡å‹ `SFTTrainer` æ‰€éœ€è¦å‡†å¤‡çš„æ¯ä¸€ä¸ªæ¨¡å—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048 # æ•°æ®é›†çš„æœ€å¤§é•¿åº¦åºåˆ—\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction, \n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡è°ƒç”¨ `Trainer` å®ä¾‹ä¸Šçš„ `train()` æ–¹æ³•æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒ\n",
    "trainer.train() # tqdmå…³é—­åå°†ä¸æ˜¾ç¤ºè¿›åº¦æ¡ä¿¡æ¯\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ä½¿ç”¨ Flash Attention çš„è®­ç»ƒè¿‡ç¨‹åœ¨ `g5.2xlarge` ä¸ŠèŠ±è´¹äº† 03:08:00ã€‚å®ä¾‹çš„æˆæœ¬ä¸º `1,212$/h` ï¼Œæ€»æˆæœ¬ä¸º `3.7$` ã€‚\n",
    "\n",
    "ä½¿ç”¨ Flash Attention çš„è®­ç»ƒè¿‡ç¨‹åœ¨ `g5.2xlarge` ä¸ŠèŠ±è´¹äº† 02:08:00ã€‚å®ä¾‹çš„æˆæœ¬ä¸º `1,212$/h` ï¼Œæ€»æˆæœ¬ä¸º `2.6$` ã€‚\n",
    "\n",
    "ä½¿ç”¨ Flash Attention çš„ç»“æœä»¤äººæ»¡æ„ï¼Œé€Ÿåº¦æé«˜äº† 1.5 å€ï¼Œæˆæœ¬é™ä½äº† 30%ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æµ‹è¯•æ¨¡å‹ã€è¿›è¡Œæ¨ç†\n",
    "\n",
    "åœ¨è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬æƒ³è¦è¿è¡Œå’Œæµ‹è¯•æ¨¡å‹ã€‚æˆ‘ä»¬ä¼šä½¿ç”¨ `peft` å’Œ `transformers` å°† LoRA é€‚é…å™¨åŠ è½½åˆ°æ¨¡å‹ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_flash_attention:\n",
    "    # åœæ­¢ flash attention\n",
    "    from utils.llama_patch import unplace_flash_attn_with_attn\n",
    "    unplace_flash_attn_with_attn()\n",
    "    \n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "args.output_dir = \"llama-7-int4-dolly\"\n",
    "\n",
    "# åŠ è½½åŸºç¡€LLMæ¨¡å‹ä¸åˆ†è¯å™¨\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ") \n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æ¥å†æ¬¡ç”¨éšæœºæ ·æœ¬åŠ è½½ä¸€æ¬¡æ•°æ®é›†ï¼Œè¯•ç€æ¥ç”Ÿæˆä¸€æ¡æŒ‡ä»¤ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# ä»hubåŠ è½½æ•°æ®é›†å¹¶å¾—åˆ°ä¸€ä¸ªæ ·æœ¬\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "sample = dataset[randrange(len(dataset))]\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    "\n",
    "### Input:\n",
    "{sample['response']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "print(f\"Prompt:\\n{sample['response']}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"Ground truth:\\n{sample['instruction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¤ªå¥½äº†ï¼æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥å·¥ä½œäº†ï¼å¦‚æœæƒ³è¦åŠ é€Ÿæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [Text Generation Inference](https://github.com/huggingface/text-generation-inference) éƒ¨ç½²å®ƒã€‚å› æ­¤æˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬é€‚é…å™¨çš„å‚æ•°åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­å»ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    ") \n",
    "\n",
    "# åˆå¹¶ LoRA ä¸ base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# ä¿å­˜åˆå¹¶åçš„æ¨¡å‹\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "\n",
    "# pushåˆå¹¶çš„æ¨¡å‹åˆ°hubä¸Š\n",
    "# merged_model.push_to_hub(\"user/repo\")\n",
    "# tokenizer.push_to_hub(\"user/repo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
